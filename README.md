# PyLama - Python Code Generation with Ollama

PyLama is a Python tool that leverages Ollama's language models to generate and execute Python code. It simplifies the process of writing and running Python scripts by handling dependency management and code execution automatically.

## Features

- üöÄ **AI-Powered Code Generation** - Generate Python code using Ollama's language models
- üîÑ **Automatic Dependency Management** - Automatically detects and installs required Python packages
- üõ† **Code Execution** - Run generated code in a controlled environment
- üîç **Error Handling** - Automatic error detection and debugging suggestions
- üì¶ **Modular Architecture** - Separated components for better maintainability

## Prerequisites

- Python 3.8+
- [Ollama](https://ollama.com/) installed and running locally
- At least one Ollama model (e.g., llama3, llama2, mistral)

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/py-lama.git
   cd py-lama/pylama
   ```

2. Install the required Python packages:
   ```bash
   pip install -r requirements.txt
   ```

3. Ensure Ollama is running:
   ```bash
   ollama serve
   ```

## Usage

### Basic Usage

```bash
python pylama.py
```

### Command Line Options

- `--model`: Specify which Ollama model to use (default: llama3)
- `--debug`: Enable debug logging
- `--output`: Specify output file for generated code

### Example

```bash
python pylama.py --model llama3 --output my_script.py
```

## Project Structure

- `pylama.py`: Main script
- `OllamaRunner.py`: Handles communication with Ollama API
- `dependency_manager.py`: Manages Python package dependencies
- `sandbox.py`: Provides a safe environment for code execution
- `models.sh`: Script to manage Ollama models

## How It Works

1. The tool analyzes your prompt and generates Python code using the specified Ollama model
2. It extracts import statements to identify required dependencies
3. It checks for and installs any missing dependencies
4. The generated code is executed in a controlled environment
5. Any errors are caught and can be used to regenerate the code

## Configuration

Create a `.env` file in the project root to customize behavior:

```env
OLLAMA_MODEL=llama3
LOG_LEVEL=INFO
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Wymagania

- Python 3.8+ (skrypt wykryje i zainstaluje zale≈ºno≈õci)
- [Ollama](https://ollama.com/download)
- Co najmniej jeden model Ollama (np. tinyllama, llama3, qwen, phi)

## Szybki start

1. **Pobierz i utw√≥rz skrypt:**

   ```bash
   # Zapisz skrypt jako setup.sh
   chmod +x setup.sh
   ```

2. **Uruchom z pe≈ÇnƒÖ konfiguracjƒÖ:**

   ```bash
   ./setup.sh
   ```

   Skrypt automatycznie:
   - Sprawdzi i zainstaluje wymagane pakiety Python
   - Sprawdzi instalacjƒô Ollama
   - Pomo≈ºe w konfiguracji modeli
   - Uruchomi serwer API

3. **Lub tylko uruchom serwer (je≈õli ju≈º skonfigurowany):**

   ```bash
   ./setup.sh --run
   ```

## Dostƒôpne opcje

Skrypt oferuje kilka przydatnych opcji:

```bash
# Wy≈õwietlenie pomocy
./setup.sh --help

# Tylko konfiguracja ≈õrodowiska (bez uruchamiania)
./setup.sh --setup

# Tylko uruchomienie serwera
./setup.sh --run

# Uruchomienie na niestandardowym porcie
./setup.sh --run --port 8080

# ZarzƒÖdzanie modelami Ollama
./setup.sh --models

# Sprawdzenie wymaga≈Ñ systemowych
./setup.sh --check
```

## Interfejs webowy

Po uruchomieniu serwera, interfejs webowy jest dostƒôpny pod adresem:

```
http://localhost:5001
```

Interfejs pozwala na:
- Zadawanie pyta≈Ñ do modelu
- Zmianƒô parametr√≥w generowania
- PrzeglƒÖdanie dostƒôpnych modeli

## API REST

Serwer udostƒôpnia nastƒôpujƒÖce endpointy:

### `POST /ask`

Wysy≈Ça zapytanie do modelu Ollama.

**≈ªƒÖdanie**:
```json
{
  "prompt": "Co to jest Python?",
  "temperature": 0.7,
  "max_tokens": 1000
}
```

**Odpowied≈∫**:
```json
{
  "response": "Python to wysokopoziomowy, interpretowany jƒôzyk programowania..."
}
```

### `GET /models`

Pobiera listƒô dostƒôpnych modeli Ollama.

**Odpowied≈∫**:
```json
{
  "models": [
    {
      "name": "tinyllama:latest",
      "size": 1640,
      "current": true
    },
    {
      "name": "llama3:latest",
      "size": 3827,
      "current": false
    }
  ]
}
```

### `POST /echo`

Proste narzƒôdzie do testowania dzia≈Çania serwera.
![img.png](img.png)
**≈ªƒÖdanie**:
```json
{
  "message": "Test"
}
```

**Odpowied≈∫**:
```json
{
  "response": "Otrzymano: Test"
}
```

## U≈ºywanie z cURL

```bash
# Zapytanie do modelu
curl -X POST -H "Content-Type: application/json" \
     -d '{"prompt":"Co to jest Python?"}' \
     http://localhost:5001/ask

# Pobranie listy modeli
curl http://localhost:5001/models

# Test echo
curl -X POST -H "Content-Type: application/json" \
     -d '{"message":"Test"}' \
     http://localhost:5001/echo
```

## Plik konfiguracyjny .env

Skrypt tworzy plik `.env` z ustawieniami, kt√≥re mo≈ºesz edytowaƒá:

```
# Konfiguracja modelu Ollama
MODEL_NAME="tinyllama:latest"

# Konfiguracja serwera
OLLAMA_URL="http://localhost:11434"
SERVER_PORT=5001

# Parametry generowania
TEMPERATURE=0.7
MAX_TOKENS=1000
```

## Obs≈Çugiwane modele

Skrypt dzia≈Ça z wszystkimi modelami dostƒôpnymi w Ollama. Oto szczeg√≥≈Çowa lista najpopularniejszych modeli:

| Model | Rozmiar | Przeznaczenie |
|-------|---------|---------------|
| **llama3** | 8B | Og√≥lnego przeznaczenia, dobry do wiƒôkszo≈õci zada≈Ñ |
| **phi3** | 3.8B | Szybki, dobry do prostszych zada≈Ñ, zoptymalizowany pod kƒÖtem kodu |
| **mistral** | 7B | Og√≥lnego przeznaczenia, efektywny energetycznie |
| **gemma** | 7B | Dobry do zada≈Ñ jƒôzyka naturalnego i kreatywnego pisania |
| **tinyllama** | 1.1B | Bardzo szybki, idealny dla s≈Çabszych urzƒÖdze≈Ñ |
| **qwen** | 7-14B | Dobry w analizie tekstu, wsparcie dla jƒôzyk√≥w azjatyckich |
| **llava** | 7-13B | Multimodalny z obs≈ÇugƒÖ obraz√≥w - pozwala na analizƒô obraz√≥w i tekstu |
| **codellama** | 7-34B | Wyspecjalizowany model do kodowania - ≈õwietny do generowania i analizy kodu |
| **vicuna** | 7-13B | Wytrenowany na konwersacjach, dobry do dialog√≥w |
| **falcon** | 7-40B | Szybki i efektywny, dobry stosunek wydajno≈õci do rozmiaru |
| **orca-mini** | 3B | Dobry do podstawowych zada≈Ñ NLP |
| **wizardcoder** | 13B | Stworzony do zada≈Ñ zwiƒÖzanych z kodem |
| **llama2** | 7-70B | Poprzednia generacja modelu Meta, sprawdzony w r√≥≈ºnych zastosowaniach |
| **stablelm** | 3-7B | Dobry do generowania tekstu i dialog√≥w |
| **dolphin** | 7B | Koncentruje siƒô na naturalno≈õci dialog√≥w |
| **neural-chat** | 7-13B | Zoptymalizowany pod kƒÖtem urzƒÖdze≈Ñ Intel |
| **starling** | 7B | Mniejszy ale skuteczny, zoptymalizowany pod kƒÖtem jako≈õci odpowiedzi |
| **openhermes** | 7B | Dobra dok≈Çadno≈õƒá, postƒôpowanie zgodnie z instrukcjami |
| **yi** | 6-34B | Zaawansowany model wielojƒôzyczny |

### Wyb√≥r rozmiaru modelu

Przy wyborze w≈Çasnego modelu, warto rozwa≈ºyƒá r√≥≈ºne rozmiary:

- **Mini (1-3B)**: Bardzo ma≈Çe modele - szybkie, ale ograniczone mo≈ºliwo≈õci
- **Small (3-7B)**: Ma≈Çe modele - dobry kompromis szybko≈õƒá/jako≈õƒá
- **Medium (8-13B)**: ≈örednie modele - lepsze odpowiedzi, wymaga wiƒôcej RAM
- **Large (30-70B)**: Du≈ºe modele - najlepsza jako≈õƒá, wysokie wymagania sprzƒôtowe

### Wymagania sprzƒôtowe

Poni≈ºej orientacyjne wymagania sprzƒôtowe dla r√≥≈ºnych rozmiar√≥w modeli:

| Rozmiar modelu | Minimalna ilo≈õƒá RAM | Zalecana ilo≈õƒá RAM | GPU |
|----------------|---------------------|-------------------|-----|
| Mini (1-3B) | 4GB | 8GB | Opcjonalnie |
| Small (3-7B) | 8GB | 16GB | Zalecany |
| Medium (8-13B) | 16GB | 24GB | Zalecany ‚â•4GB VRAM |
| Large (30-70B) | 32GB | 64GB | Wymagany ‚â•8GB VRAM |

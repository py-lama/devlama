# pylama


# Uniwersalny Serwer Ollama

https://ollama.com/search?c=vision
https://ollama.com/search?o=newest

![GUI.png](GUI.png)

Kompletne, uniwersalne rozwiÄ…zanie do uruchamiania i zarzÄ…dzania lokalnym serwerem modeli jÄ™zykowych poprzez Ollama. DziaÅ‚a na wszystkich dystrybucjach Linuxa oraz macOS.

## Funkcje

- ğŸš€ **UniwersalnoÅ›Ä‡** - dziaÅ‚a na wszystkich dystrybucjach Linux oraz macOS
- ğŸ”„ **Automatyczna konfiguracja** - automatyczna instalacja wymaganych pakietÃ³w
- ğŸ“ **Interfejs webowy** - do testowania modeli w przeglÄ…darce
- âš™ï¸ **Åatwa konfiguracja** - przez plik .env lub parametry wiersza poleceÅ„
- ğŸŒ **Proste API REST** - do integracji z aplikacjami
- ğŸ”„ **ObsÅ‚uga wielu modeli** - z moÅ¼liwoÅ›ciÄ… Å‚atwego przeÅ‚Ä…czania
- ğŸ“Š **Zaawansowane zarzÄ…dzanie parametrami** - temperatura, max_tokens, itp.

## Wymagania

- Python 3.8+ (skrypt wykryje i zainstaluje zaleÅ¼noÅ›ci)
- [Ollama](https://ollama.com/download)
- Co najmniej jeden model Ollama (np. tinyllama, llama3, qwen, phi)

## Szybki start

1. **Pobierz i utwÃ³rz skrypt:**

   ```bash
   # Zapisz skrypt jako setup.sh
   chmod +x setup.sh
   ```

2. **Uruchom z peÅ‚nÄ… konfiguracjÄ…:**

   ```bash
   ./setup.sh
   ```

   Skrypt automatycznie:
   - Sprawdzi i zainstaluje wymagane pakiety Python
   - Sprawdzi instalacjÄ™ Ollama
   - PomoÅ¼e w konfiguracji modeli
   - Uruchomi serwer API

3. **Lub tylko uruchom serwer (jeÅ›li juÅ¼ skonfigurowany):**

   ```bash
   ./setup.sh --run
   ```

## DostÄ™pne opcje

Skrypt oferuje kilka przydatnych opcji:

```bash
# WyÅ›wietlenie pomocy
./setup.sh --help

# Tylko konfiguracja Å›rodowiska (bez uruchamiania)
./setup.sh --setup

# Tylko uruchomienie serwera
./setup.sh --run

# Uruchomienie na niestandardowym porcie
./setup.sh --run --port 8080

# ZarzÄ…dzanie modelami Ollama
./setup.sh --models

# Sprawdzenie wymagaÅ„ systemowych
./setup.sh --check
```

## Interfejs webowy

Po uruchomieniu serwera, interfejs webowy jest dostÄ™pny pod adresem:

```
http://localhost:5001
```

Interfejs pozwala na:
- Zadawanie pytaÅ„ do modelu
- ZmianÄ™ parametrÃ³w generowania
- PrzeglÄ…danie dostÄ™pnych modeli

## API REST

Serwer udostÄ™pnia nastÄ™pujÄ…ce endpointy:

### `POST /ask`

WysyÅ‚a zapytanie do modelu Ollama.

**Å»Ä…danie**:
```json
{
  "prompt": "Co to jest Python?",
  "temperature": 0.7,
  "max_tokens": 1000
}
```

**OdpowiedÅº**:
```json
{
  "response": "Python to wysokopoziomowy, interpretowany jÄ™zyk programowania..."
}
```

### `GET /models`

Pobiera listÄ™ dostÄ™pnych modeli Ollama.

**OdpowiedÅº**:
```json
{
  "models": [
    {
      "name": "tinyllama:latest",
      "size": 1640,
      "current": true
    },
    {
      "name": "llama3:latest",
      "size": 3827,
      "current": false
    }
  ]
}
```

### `POST /echo`

Proste narzÄ™dzie do testowania dziaÅ‚ania serwera.
![img.png](img.png)
**Å»Ä…danie**:
```json
{
  "message": "Test"
}
```

**OdpowiedÅº**:
```json
{
  "response": "Otrzymano: Test"
}
```

## UÅ¼ywanie z cURL

```bash
# Zapytanie do modelu
curl -X POST -H "Content-Type: application/json" \
     -d '{"prompt":"Co to jest Python?"}' \
     http://localhost:5001/ask

# Pobranie listy modeli
curl http://localhost:5001/models

# Test echo
curl -X POST -H "Content-Type: application/json" \
     -d '{"message":"Test"}' \
     http://localhost:5001/echo
```

## Plik konfiguracyjny .env

Skrypt tworzy plik `.env` z ustawieniami, ktÃ³re moÅ¼esz edytowaÄ‡:

```
# Konfiguracja modelu Ollama
MODEL_NAME="tinyllama:latest"

# Konfiguracja serwera
OLLAMA_URL="http://localhost:11434"
SERVER_PORT=5001

# Parametry generowania
TEMPERATURE=0.7
MAX_TOKENS=1000
```

## ObsÅ‚ugiwane modele

Skrypt dziaÅ‚a z wszystkimi modelami dostÄ™pnymi w Ollama. Oto szczegÃ³Å‚owa lista najpopularniejszych modeli:

| Model | Rozmiar | Przeznaczenie |
|-------|---------|---------------|
| **llama3** | 8B | OgÃ³lnego przeznaczenia, dobry do wiÄ™kszoÅ›ci zadaÅ„ |
| **phi3** | 3.8B | Szybki, dobry do prostszych zadaÅ„, zoptymalizowany pod kÄ…tem kodu |
| **mistral** | 7B | OgÃ³lnego przeznaczenia, efektywny energetycznie |
| **gemma** | 7B | Dobry do zadaÅ„ jÄ™zyka naturalnego i kreatywnego pisania |
| **tinyllama** | 1.1B | Bardzo szybki, idealny dla sÅ‚abszych urzÄ…dzeÅ„ |
| **qwen** | 7-14B | Dobry w analizie tekstu, wsparcie dla jÄ™zykÃ³w azjatyckich |
| **llava** | 7-13B | Multimodalny z obsÅ‚ugÄ… obrazÃ³w - pozwala na analizÄ™ obrazÃ³w i tekstu |
| **codellama** | 7-34B | Wyspecjalizowany model do kodowania - Å›wietny do generowania i analizy kodu |
| **vicuna** | 7-13B | Wytrenowany na konwersacjach, dobry do dialogÃ³w |
| **falcon** | 7-40B | Szybki i efektywny, dobry stosunek wydajnoÅ›ci do rozmiaru |
| **orca-mini** | 3B | Dobry do podstawowych zadaÅ„ NLP |
| **wizardcoder** | 13B | Stworzony do zadaÅ„ zwiÄ…zanych z kodem |
| **llama2** | 7-70B | Poprzednia generacja modelu Meta, sprawdzony w rÃ³Å¼nych zastosowaniach |
| **stablelm** | 3-7B | Dobry do generowania tekstu i dialogÃ³w |
| **dolphin** | 7B | Koncentruje siÄ™ na naturalnoÅ›ci dialogÃ³w |
| **neural-chat** | 7-13B | Zoptymalizowany pod kÄ…tem urzÄ…dzeÅ„ Intel |
| **starling** | 7B | Mniejszy ale skuteczny, zoptymalizowany pod kÄ…tem jakoÅ›ci odpowiedzi |
| **openhermes** | 7B | Dobra dokÅ‚adnoÅ›Ä‡, postÄ™powanie zgodnie z instrukcjami |
| **yi** | 6-34B | Zaawansowany model wielojÄ™zyczny |

### WybÃ³r rozmiaru modelu

Przy wyborze wÅ‚asnego modelu, warto rozwaÅ¼yÄ‡ rÃ³Å¼ne rozmiary:

- **Mini (1-3B)**: Bardzo maÅ‚e modele - szybkie, ale ograniczone moÅ¼liwoÅ›ci
- **Small (3-7B)**: MaÅ‚e modele - dobry kompromis szybkoÅ›Ä‡/jakoÅ›Ä‡
- **Medium (8-13B)**: Åšrednie modele - lepsze odpowiedzi, wymaga wiÄ™cej RAM
- **Large (30-70B)**: DuÅ¼e modele - najlepsza jakoÅ›Ä‡, wysokie wymagania sprzÄ™towe

### Wymagania sprzÄ™towe

PoniÅ¼ej orientacyjne wymagania sprzÄ™towe dla rÃ³Å¼nych rozmiarÃ³w modeli:

| Rozmiar modelu | Minimalna iloÅ›Ä‡ RAM | Zalecana iloÅ›Ä‡ RAM | GPU |
|----------------|---------------------|-------------------|-----|
| Mini (1-3B) | 4GB | 8GB | Opcjonalnie |
| Small (3-7B) | 8GB | 16GB | Zalecany |
| Medium (8-13B) | 16GB | 24GB | Zalecany â‰¥4GB VRAM |
| Large (30-70B) | 32GB | 64GB | Wymagany â‰¥8GB VRAM |

# Manual Installation Guide for SpeakLeash/bielik Models with Ollama

This guide provides detailed instructions for manually installing and configuring SpeakLeash/bielik models for use with PyLama, rather than relying on automatic model selection.

## Manual Deployment from GGUF Source

When the SpeakLeash/bielik model isn't available in the Ollama registry, you can manually install it following these steps:

### 1. Download the GGUF File

First, download the model file from Hugging Face (note: this is a large file, approximately 1.5GB):

```bash
# Create a directory for the model
mkdir -p ~/models/bielik
cd ~/models/bielik

# Download the model file (Q8_0 version - higher quality)
wget https://huggingface.co/speakleash/Bielik-1.5B-v3.0-Instruct-GGUF/resolve/main/Bielik-1.5B-v3.0-Instruct.Q8_0.gguf

# Or download the FP16 version (larger file but higher quality)
# wget https://huggingface.co/speakleash/Bielik-1.5B-v3.0-Instruct-GGUF/resolve/main/Bielik-1.5B-v3.0-Instruct-fp16.gguf
```

### 2. Create a Modelfile

After the download completes, create a Modelfile in the same directory:

```bash
cat > Modelfile << 'EOF'
FROM ./Bielik-1.5B-v3.0-Instruct.Q8_0.gguf
PARAMETER num_ctx 4096
SYSTEM """
Poland-optimized NLU model with constitutional AI constraints
"""
EOF
```

### 3. Create the Model in Ollama

Build the model with Ollama:

```bash
ollama create bielik-custom -f ./Modelfile
```

### 4. Verify the Model

Verify that the model was created successfully:

```bash
ollama list | grep bielik
```

### 5. Use with PyLama

Now you can use the custom model with PyLama:

```bash
pylama --model bielik-custom "print hello world"
```

## Alternative Installation Method: Hugging Face CLI

If you prefer using the Hugging Face CLI for more reliable downloads:

```bash
# Install the Hugging Face CLI
pip install huggingface_hub

# Create a directory for the model
mkdir -p ~/models/bielik-cli

# Download the model (this will show a progress bar)
huggingface-cli download speakleash/Bielik-1.5B-v3.0-Instruct-GGUF Bielik-1.5B-v3.0-Instruct.Q8_0.gguf --local-dir ~/models/bielik-cli

# Create a Modelfile in the download directory
cd ~/models/bielik-cli
cat > Modelfile << 'EOF'
FROM ./Bielik-1.5B-v3.0-Instruct.Q8_0.gguf
PARAMETER num_ctx 4096
SYSTEM """
Poland-optimized NLU model with constitutional AI constraints
"""
EOF

# Create the model in Ollama
ollama create bielik-custom -f ./Modelfile
```

## Integrating with PyLama's Automatic Model Selection

After manually installing the model, you can configure PyLama to use it as a fallback option. There are two reliable ways to do this:

### Method 1: Create a .env file

Create or edit the `.env` file in your PyLama directory:

```bash
# Create or edit the .env file
cat > /home/tom/github/py-lama/pylama/.env << 'EOF'
OLLAMA_MODEL=bielik-custom
OLLAMA_FALLBACK_MODELS=bielik-custom,phi:2.7b,tinyllama:latest
OLLAMA_AUTO_SELECT_MODEL=true
OLLAMA_TIMEOUT=60
EOF
```

### Method 2: Set environment variables in your shell configuration

Add these lines to your `~/.bashrc` or `~/.zshrc` file:

```bash
# Add to your shell configuration file
export OLLAMA_MODEL=bielik-custom
export OLLAMA_FALLBACK_MODELS=bielik-custom,phi:2.7b,tinyllama:latest
export OLLAMA_AUTO_SELECT_MODEL=true
export OLLAMA_TIMEOUT=60
```

Then reload your shell configuration:

```bash
source ~/.bashrc  # or source ~/.zshrc if using zsh
```

With this configuration:

1. PyLama will try to use `bielik-custom` first
2. If that fails, it will automatically try the fallback models
3. The progress spinner will show which model is being used
4. The API timeout is set to 60 seconds to avoid timeout issues with larger models

## Performance Optimization

### Quantization Options

The model is available in different quantization levels:

| Version | File Size | Quality |
|---------|-----------|----------|
| Q8_0    | ~1.5GB    | Good balance of size/quality |
| FP16    | ~3GB      | Highest quality, larger file |

Choose the quantization that best fits your system's capabilities.

### GPU Acceleration

For better performance with GPU:

```bash
export CUDA_VISIBLE_DEVICES=0  # Use first GPU
export OLLAMA_GPU_LAYERS=24     # Number of layers to offload to GPU
```

## Troubleshooting

### Model Not Found

If you see an error like:

```
Model SpeakLeash/bielik-1.5b-v3.0-instruct-gguf not found in Ollama.
```

This means you need to manually install the model as described above, rather than relying on the automatic model selection.

### Direct Ollama Pull Doesn't Work

We've verified that the following command does not work for this model:

```bash
ollama pull speakleash/bielik-1.5b-v3.0-instruct
```

You must use the manual installation method described in this guide.

### Download Taking Too Long

If the download is taking too long:

1. Try using a different network connection
2. Use the Hugging Face CLI method which provides a progress bar
3. Consider using a smaller model like `phi:2.7b` which is already available in your Ollama installation

### API Timeout Issues

If you experience API timeout errors when using the model:

```
Error querying Ollama API: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=30)
```

This is a common issue with larger models. To fix it:

1. Increase the API timeout by setting the `OLLAMA_TIMEOUT` environment variable as shown in the configuration section above
2. Ensure your system has enough RAM and CPU resources available
3. Consider using a smaller model or a model with lower quantization for faster responses

The default timeout is 30 seconds, which may not be enough for larger models or complex prompts.

## Advanced Configuration

### Custom System Prompt

You can customize the model's behavior by modifying the system prompt in the Modelfile:

```
SYSTEM """
You are an AI assistant specialized in Polish legal documentation.
Follow these constraints:
1. Never provide financial advice
2. Reject requests for medical information
3. Cite legal sources for all statements
"""
```

### Network Configuration

Configure Ollama's API endpoint for remote access:

```yaml
# /etc/ollama/config.yaml
host: 0.0.0.0
port: 11434
ssl_cert: /etc/ssl/ollama.crt
ssl_key: /etc/ssl/ollama.key
```

## References

- [Hugging Face SpeakLeash Models](https://huggingface.co/speakleash)
- [Bielik-1.5B-v3.0-Instruct-GGUF Model](https://huggingface.co/speakleash/Bielik-1.5B-v3.0-Instruct-GGUF)
- [Ollama Modelfile Documentation](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
- [GGUF Format Specifications](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
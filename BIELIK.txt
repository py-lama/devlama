# Manual Installation Guide for SpeakLeash/bielik Models with Ollama

This guide provides detailed instructions for manually installing and configuring SpeakLeash/bielik models for use with PyLama. Note that PyLama now supports automatic installation of these models, but this guide is useful if you prefer manual installation or need to troubleshoot issues.

## Automatic Installation (Recommended)

PyLama now supports automatic installation of SpeakLeash/bielik models. Simply run:

```bash
pylama --model SpeakLeash/bielik-1.5b-v3.0-instruct-gguf "print hello world"
```

This will:
1. Download the model file from Hugging Face
2. Create a custom Modelfile with appropriate parameters
3. Create the model in Ollama with a unique name
4. Update your environment settings to use the new model
5. Automatically reuse the model in future runs without downloading again

Example output:
```
Found existing Bielik model installation: bielik-custom-1747866289:latest
Using existing model instead of downloading again.
Increased API timeout to 120 seconds for Bielik model.

Generated Python code:
----------------------------------------
#!/bin/env python3
import sys, os

# Function to check if the operating system is Linux
def is_linux():
    return os.name == 'posix' and sys.platform == 'linux'

if not is_linux():
    print("This code requires a Linux operating system to run.")
    exit()

# Hello World function
def hello_world():
    print("Hello, World!")

if __name__ == "__main__":
    # Call the hello_world function
    hello_world()
----------------------------------------
```

## Manual Deployment from GGUF Source

If you prefer to manually install the model or if automatic installation fails, follow these steps:

### 1. Download the GGUF File

First, download the model file from Hugging Face (note: this is a large file, approximately 1.5GB):

```bash
# Create a directory for the model
mkdir -p ~/models/bielik
cd ~/models/bielik

# Download the model file (Q8_0 version - higher quality)
wget https://huggingface.co/speakleash/Bielik-1.5B-v3.0-Instruct-GGUF/resolve/main/Bielik-1.5B-v3.0-Instruct.Q8_0.gguf

# Or download the FP16 version (larger file but higher quality)
# wget https://huggingface.co/speakleash/Bielik-1.5B-v3.0-Instruct-GGUF/resolve/main/Bielik-1.5B-v3.0-Instruct-fp16.gguf
```

### 2. Create a Modelfile

After the download completes, create a Modelfile in the same directory:

```bash
cat > Modelfile << 'EOF'
FROM ./Bielik-1.5B-v3.0-Instruct.Q8_0.gguf
PARAMETER num_ctx 4096
SYSTEM """
Poland-optimized NLU model with constitutional AI constraints
"""
EOF
```

### 3. Create the Model in Ollama

Build the model with Ollama:

```bash
ollama create bielik-custom -f ./Modelfile
```

### 4. Verify the Model

Verify that the model was created successfully:

```bash
ollama list | grep bielik
```

### 5. Use with PyLama

Now you can use the custom model with PyLama:

```bash
pylama --model bielik-custom "print hello world"
```

## Alternative Installation Method: Hugging Face CLI

If you prefer using the Hugging Face CLI for more reliable downloads:

```bash
# Install the Hugging Face CLI
pip install huggingface_hub

# Create a directory for the model
mkdir -p ~/models/bielik-cli

# Download the model (this will show a progress bar)
huggingface-cli download speakleash/Bielik-1.5B-v3.0-Instruct-GGUF Bielik-1.5B-v3.0-Instruct.Q8_0.gguf --local-dir ~/models/bielik-cli

# Create a Modelfile in the download directory
cd ~/models/bielik-cli
cat > Modelfile << 'EOF'
FROM ./Bielik-1.5B-v3.0-Instruct.Q8_0.gguf
PARAMETER num_ctx 4096
SYSTEM """
Poland-optimized NLU model with constitutional AI constraints
"""
EOF

# Create the model in Ollama
ollama create bielik-custom -f ./Modelfile
```

## Integrating with PyLama's Automatic Model Selection

After manually installing the model, you can configure PyLama to use it as a fallback option. There are two reliable ways to do this:

### Method 1: Create a .env file

Create or edit the `.env` file in your PyLama directory:

```bash
# Create or edit the .env file
cat > /home/tom/github/py-lama/pylama/.env << 'EOF'
OLLAMA_MODEL=bielik-custom
OLLAMA_FALLBACK_MODELS=bielik-custom,phi:2.7b,tinyllama:latest
OLLAMA_AUTO_SELECT_MODEL=true
OLLAMA_TIMEOUT=120
EOF
```

### Method 2: Set environment variables in your shell configuration

Add these lines to your `~/.bashrc` or `~/.zshrc` file:

```bash
# Add to your shell configuration file
export OLLAMA_MODEL=bielik-custom
export OLLAMA_FALLBACK_MODELS=bielik-custom,phi:2.7b,tinyllama:latest
export OLLAMA_AUTO_SELECT_MODEL=true
export OLLAMA_TIMEOUT=120
```

Then reload your shell configuration:

```bash
source ~/.bashrc  # or source ~/.zshrc if using zsh
```

With this configuration:

1. PyLama will try to use `bielik-custom` first
2. If that fails, it will automatically try the fallback models
3. The progress spinner will show which model is being used
4. The API timeout is set to 120 seconds to avoid timeout issues with larger models

## Performance Optimization

### Quantization Options

The model is available in different quantization levels:

| Version | File Size | Quality |
|---------|-----------|----------|
| Q8_0    | ~1.5GB    | Good balance of size/quality |
| FP16    | ~3GB      | Highest quality, larger file |

Choose the quantization that best fits your system's capabilities.

### GPU Acceleration

For better performance with GPU:

```bash
# Set Ollama to use GPU
export OLLAMA_USE_GPU=true
```

## Troubleshooting

### Model Not Found

If you see an error like this:

```
Model 'SpeakLeash/bielik-1.5b-v3.0-instruct-gguf' not found in Ollama. Available models: ['codellama:7b', 'phi:2.7b']
```

Possible solutions:

1. Make sure you've enabled automatic model installation: `export OLLAMA_AUTO_INSTALL_MODEL=true`
2. Check if you have sufficient disk space for the model download (~1.5GB)
3. Consider using a smaller model like `phi:2.7b` which is already available in your Ollama installation

### API Timeout Issues

If you experience API timeout errors when using the model:

```
Error querying Ollama API: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=30)
```

Possible solutions:

1. Increase the timeout value: `export OLLAMA_TIMEOUT=120`
2. Ensure Ollama is running: `ollama serve`
3. Check if your system has sufficient resources to run the model
4. Try using a smaller model if your system has limited resources

### Duplicate Model Downloads

If PyLama is downloading the model again even though you've already installed it:

1. The automatic detection should now prevent this, as it looks for any model name starting with `bielik-custom-`
2. If you still experience issues, manually set the model name in your environment: `export OLLAMA_MODEL=bielik-custom-1234567890` (use the actual model name from `ollama list`)

## Additional Resources

- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
- [SpeakLeash Models on Hugging Face](https://huggingface.co/speakleash)
- [PyLama GitHub Repository](https://github.com/py-lama/pylama)